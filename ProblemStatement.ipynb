{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9961a9f6",
   "metadata": {},
   "source": [
    "# Data gathering, problem statement, stakeholders, KPIs\n",
    "\n",
    "## Data gathering\n",
    "\n",
    "For each headline listed on [fivethirtyeight/politics/features](https://fivethirtyeight.com/politics/features/), at the top, then under \"Latest Politics\", we store the type of post, its title, its url, the author(s), the date and time posted, a list of the article's tags, according to 538, and the number of comments.  We scrape all headlines from the features pages, except for the live-blogs, which don't have any comments.  The hardest part to scrape is the number of comments, since 538 uses the Facebook comments plugin.  First we import the necessary python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6eebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the html\n",
    "import requests\n",
    "\n",
    "# Parse the html\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Render JavaScript to scrape the comments\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Delays and time for execution of the code\n",
    "import time # for debugging\n",
    "\n",
    "# Get the date and time\n",
    "from datetime import datetime\n",
    "\n",
    "# For splitting using more than one delimiter\n",
    "import re\n",
    "\n",
    "# Makes a csv file quickly\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e0798",
   "metadata": {},
   "source": [
    "Since scraping the comments is the hardest part, we write a function that will do it.  It only works for posts from [fivethirtyeight.com/features](https://fivethirtyeight.com/features).  The function takes some time each time it's run, so there are debugging commands to track its progress.  Any line in the code with the comment \"for debugging\" can be commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f31600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is the url of one of the features posts on fivethirtyeight.com/politics/features pages.\n",
    "# Output is the number of comments on the post.\n",
    "def num_comments_538_post(url):\n",
    "    # Start the timer to time the execution of each iteration of this function\n",
    "    start = time.time() # for debugging\n",
    "    # Function only works when the input is a features article from fivethirtyeight.com\n",
    "    print(\"Comments scraping current url:\", url) # for debugging\n",
    "    # Create a webdriver object with selenium that will get the required html    \n",
    "    # Here Chrome will be used, but modifications to the code for other browsers exist\n",
    "    driver = webdriver.Chrome()\n",
    "    # Open the 538 webpage after 10 seconds\n",
    "    time.sleep(10)\n",
    "    driver.get(url)\n",
    "    # Click the expand comments button\n",
    "    driver.find_element(By.CLASS_NAME, \"fte-expandable-icon\").click()\n",
    "    # Execute the JavaScript after clicking the button\n",
    "    article_html = driver.execute_script(\"return document.documentElement.outerHTML;\")\n",
    "    # Close the 538 webpage\n",
    "    driver.quit()\n",
    "    # Parse the html\n",
    "    article_soup = BeautifulSoup(article_html, \"lxml\")\n",
    "    # Find the iframe corresponding to the comments\n",
    "    comments_frame = article_soup.find('iframe', attrs = {'data-testid':\"fb:comments Facebook Social Plugin\"})\n",
    "    # Get the source attribute in the iframe \n",
    "    comments_url = comments_frame['src']\n",
    "    # Redefine the webdriver object (needed to avoid errors)\n",
    "    driver = webdriver.Chrome()\n",
    "    # Open the Facebook comments plugin url\n",
    "    driver.get(comments_url)\n",
    "    # Execute the JavaScript on that page\n",
    "    comments_html = driver.execute_script(\"return document.documentElement.outerHTML;\")\n",
    "    # Close the comments page\n",
    "    #driver.quit()\n",
    "    # Parse the rendered code\n",
    "    comments_soup = BeautifulSoup(comments_html,\"lxml\")\n",
    "    # Find the element that contains the number of comments\n",
    "    number = comments_soup.find('span',  attrs = {'class':\"_50f7\"}).text.strip(\" comments\")\n",
    "    print(\"The number of comments is \"+str(number)+\".\") # for debugging\n",
    "    # End the timer\n",
    "    end = time.time() # for debugging\n",
    "    print(\"Time elapsed:\", end-start, \"seconds\\n\") # for debugging \n",
    "    return number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fffedb1",
   "metadata": {},
   "source": [
    "Now we extract the desired data from each headline under \"Latest Politics\", including the main article, on the [538 features](https://www.fivethirtyeight.com/politics/features) page(s).  In the following code, the authors and tags are originally stored as lists.  However, when we convert all the data into a data frame later, we will need the data to have the right shape -- we need it to be a list of lists, with no additional nested lists.  For authors and tags we turn the list into a string where the items are separated by semicolons instead of commas.  This will make it possible to create a `.csv` file with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713cc01a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the date and time to put in the name of the output file\n",
    "now = datetime.now()\n",
    "\n",
    "# Set timer for full execution\n",
    "start_full = time.time() # for debugging\n",
    "\n",
    "# How many pages of features to extract data from\n",
    "features_num_pages = 110 #input(\"How many features pages to scrape?  Each has about 10 posts.  \")\n",
    "#print(\"This code will scrape data from\", features_num_pages, \"page(s) worth of posts in 538's politics/features section.\\n\") # for debugging\n",
    "\n",
    "# Here is where all the data will go\n",
    "posts = []\n",
    "# Get the data for each post\n",
    "for i in range(features_num_pages): \n",
    "    print(\"\\nPage \"+str(i+1)+\"...\\n\") # for debugging\n",
    "    # Get the html for each headline\n",
    "    features_url = \"https://fivethirtyeight.com/politics/features/page/\"+str(i+1)\n",
    "    features_html = requests.get(features_url)\n",
    "    # Parse the html\n",
    "    features_soup = BeautifulSoup(features_html.content)\n",
    "    # Gather the data for each of articles\n",
    "    features = features_soup.find_all('h2', attrs = {'class':[\"article-title entry-title\", \"title entry-title\"]})\n",
    "    for post in features:\n",
    "        # Get post title from the features page\n",
    "        title = post.a.text.strip('\\n''\\t')\n",
    "        # Get post url from the features page\n",
    "        url = post.find('a').get('href')\n",
    "        # Screen for live blogs, which don't have comments\n",
    "        if \"live-blog\" in url:\n",
    "            continue\n",
    "        # Go to the url to get more data\n",
    "        post_code = requests.get(url)\n",
    "        post_soup = BeautifulSoup(post_code.content)\n",
    "        # Get author(s)\n",
    "        author_bios = post_soup.find_all('div', attrs = {'class':\"mini-bio\"})\n",
    "        if author_bios == []:\n",
    "            authors = \"None/All\"\n",
    "        else:    \n",
    "            authors_list = []\n",
    "            for author in author_bios:\n",
    "                # Extract the author name\n",
    "                to_extract = author.p.text\n",
    "                to_extract_list = re.split(\" is | reports\", to_extract)\n",
    "                authors_list.append(to_extract_list[0])\n",
    "            authors = str(authors_list).replace(\",\", \";\").strip(\"[\" \"]\").replace(\"\\'\", \"\")    \n",
    "        # Get date and time of post\n",
    "        date = post_soup.find('time').text.strip('\\n''\\t')\n",
    "        # Get tags\n",
    "        tags_list = []\n",
    "        for tag in post_soup.find_all('a', attrs = {'class':\"tag\"}):\n",
    "            tags_list.append(tag.text.split(\" (\")[0])\n",
    "        tags = str(tags_list).replace(\",\", \";\").strip(\"[\" \"]\").replace(\"\\'\", \"\")      \n",
    "        # Use the tags to get the post type\n",
    "        if \"Politics Podcast\" in tags:\n",
    "            post_type = \"podcast\"\n",
    "        else:    \n",
    "            post_type = post.find('a').get('data-content-type') \n",
    "        if post_type == None:\n",
    "            post_type = \"feature\"\n",
    "        # Change the name \"feature\" to \"article\"    \n",
    "        if post_type == \"feature\":\n",
    "            post_type = \"article\"\n",
    "        # Get number of comments\n",
    "        num_comments = num_comments_538_post(url)    \n",
    "        # Add all attributes to list\n",
    "        posts.append([post_type, title, url, authors, date, tags, num_comments])\n",
    "    if len(posts) == 1000:\n",
    "        break\n",
    "\n",
    "# End the timer for the full execution\n",
    "end_full = time.time() # for debugging\n",
    "\n",
    "# Compute time elapsed in seconds\n",
    "total_time_seconds = end_full-start_full # for debugging\n",
    "# In minutes \n",
    "total_time_minutes = total_time_seconds/60 # for debugging\n",
    "if total_time_minutes < 60: # for debugging\n",
    "    print(\"Total time elapsed =\", total_time_minutes, \"minutes\") # for debugging\n",
    "else: # for debugging\n",
    "    # In hours\n",
    "    total_time_hours = total_time_minutes/60 # for debugging\n",
    "    # Print the time elapsed in hours\n",
    "    print(\"Total time elapsed =\", total_time_hours, \"hours\") # for debugging\n",
    "\n",
    "# The data\n",
    "print(\"Number of posts scraped:\", len(posts)) # for debugging\n",
    "#posts # for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c72c09",
   "metadata": {},
   "source": [
    "Now we save the data frame to a `.csv` file to use in the data exploration phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ddb63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to make a data frame \n",
    "df = pd.DataFrame(posts)\n",
    "df.columns = [\"Post type\", \"Title\", \"Post url\", \"Author(s)\", \"Date and time posted\", \"Tags\", \"No. of comments\"]\n",
    "# Then save it as a .csv file, with the index column removed\n",
    "df.to_csv(\"ProblemStatementOutputs/\"+str(len(posts))+\"_\"+now.strftime(\"%d-%m-%Y_%H-%M-%S\")+\".csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639fefb",
   "metadata": {},
   "source": [
    "The name of the file has the form (number of posts)\\_(date)-(month)-(year in 4 digits)\\_(hour in military time)-(minute)-(seconds)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a8abd",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "\n",
    "Which 538 features posts get the most traffic?\n",
    "\n",
    "## Stakeholders\n",
    "\n",
    "News has become more polarized and sensationalized in recent years, all in the name of more clicks. This data analysis could provide some insight into what kind of articles and other content (podcasts and videos) get more traffic, without news organizations having to compromise their neutrality and factual correctness.\n",
    "\n",
    "## Key performance indicators (KPIs)\n",
    "\n",
    "- Number of comments a post gets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
